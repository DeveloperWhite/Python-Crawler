# 爬虫基础
# 准备一个网站
# url= 'http://www.baidu.com'
# from urllib.request import urlopen
#
# from urllib3 import request
#
# url = 'http://www.baidu.com'
# resp = urlopen(url) # 通过urlopen方法打开url
# print(resp.read().decode('utf-8'))
# 不是有所的网站把数据直接展示在页面源代码上的
# 第二种请求方式：
# 输入网址后，服务器会返回一个页面源代码（可能会缺少一些数据）
# 在一个特殊情况下，触发一个新的请求
# 这个新的请求，专门用来请求数据
# 服务端返回数据，浏览器执行一些脚本，把数据渲染到页面上

# A.服务器渲染全部数据
# B.服务器渲染部分数据 省钱，用户体验好，爬虫见得多（数据不在页面源代码上）京东，淘宝，拼多多
# 需要想办法看到真正加载数据的那个请求，然后提取数据

# F12查看网页
# 我们的程序拿到的是"页面源代码"，不能以Element为准，我们要以页面源代码为准
# Element：看到的代码是经过脚本，浏览器运行之后的东西（当前的状态），页面源代码和Element是不一样的，所以有时候请求的是页面源代码，导致有些数据拿不到
# Console：能够写代码，能够执行代码(JS代码)
# Sources：网页加载的时候需要用到的东西，比如图片，CSS，JS，HTML资源都有，后续经常会用到
# Network：抓包工具，可以看到所有的请求，每个请求可以理解为是数据包，这个抓包工具就能抓数据包

# 第一种情况
#输入一个url
# 浏览器会发送一个请求到服务器 -->请求
# 服务器接收到请求之后，组织一下数据，把数据返回给浏览器 -->响应

# 第二种情况
#输入一个url
# 浏览器发送请求后，服务器只是返回一个基本的html界面(没有数据)
# 当进行某种操作，或执行脚本后，可能会在发送一次请求，服务器此时就会返回数据
# 浏览器会将html界面和数据整合，一起渲染到页面上

# http协议
# 两个计算机之间为了进行流畅的沟通，设置的一个君子协议，不同的协议传输的数据格式不同
# http协议：超文本传输协议,用于浏览器和服务器之间的数据交互所遵守的协议,用的最多的用来加载网页
# http协议把一条消息分为三部份内容：无论请求还是响应，都是这三部分内容

# 请求：
# 请求行：请求方法，请求url，协议版本
# 请求头：放一些服务器需要使用的附加信息（cookie验证，token，各式各样的反扒信息）
# 空行
# 请求体：放一些请求参数

# 响应：
# 响应行：协议版本，状态码，状态码描述
# 响应头：放一些客户端要使用的附加信息（cookie验证，token，各式各样的反扒信息）
# 空行
# 响应体：服务器返回真正客户端要用的内容，页面源代码

# 重点内容:
# User-Agent：用户的助手：表示用户在访问这个网站时所用的设备（存在于请求头之中）
# 服务器返回的响应头中会有cookie的信息
# 请求方式：（实际上有9种）
# get：显示请求，能在地址栏直接看到get提交的数据，浏览器直接输入网址的位置（这一次请求是get请求），超链接一般情况下是get请求
# post：隐示请求（post提交的数据一般在地址栏看不见），表单（登录，注册，输入密码的地方）

# requests模块入门(重点):第三方模块,优势:比urllib简单
import requests
# request核心功能:发起请求

#第一个案例：搜狗搜索
content=input("请输入要搜索的内容：")
url=f"https://www.sogou.com/web?query={content}"
# 所以要进行伪装，设计一个UA信息，伪装成浏览器，header是自己定义的
headers={
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
}
# 发送请求，第一个headers是固定的，第二个headers是自定义的，上面定义的是啥，这里就是啥
resp=requests.get(url,headers=headers) #发送get请求
# request.post() #发送post请求
print(resp) #<Response [200]>
print(resp.text) #获取响应体内容
print(resp.request.headers) #获取请求头中的UA信息

# 第二个案例：百度翻译
# post请求在url上看不到参数
